# å®ŒæˆåŸºæœ¬çš„UIç•Œé¢ï¼Œè°ƒè¯•ä½¿å¾—LLMå›ç­”çš„å‡†ç¡®åº¦æé«˜
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_openai import ChatOpenAI
import gradio as gr
import os
import time
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
import getpass

# è°ƒç”¨LangChain-smith
os.environ['LANGCHAIN_API_KEY'] = 'ä½ çš„LangSmith-API-KEY'
os.environ['LANGCHAIN_PROJECT'] = 'RAG-Application'
os.environ['LANGCHAIN_TRACING_V2'] = 'true'
os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'

# æ–‡ä»¶åŠ è½½ï¼Œç›´æ¥åŠ è½½æœ¬åœ°bookæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨æ‹†åˆ†å™¨å°†å…¶æ‹†åˆ†
def load_documents(directory):
    # silent_errorså¯ä»¥è·³è¿‡ä¸èƒ½è§£ç çš„å†…å®¹
    text_loader_kwargs = {'autodetect_encoding': True}
    loader = DirectoryLoader(directory, show_progress=True, silent_errors=True, loader_kwargs=text_loader_kwargs)
    documents = loader.load()

    # åŠ è½½æ–‡æ¡£åï¼Œè¦ä½¿å¾—å†…å®¹å˜å¾—æ˜“äºllmåŠ è½½ï¼Œå°±å¿…é¡»æŠŠé•¿æ–‡æœ¬åˆ‡å‰²æˆä¸€ä¸ªä¸ªçš„å°æ–‡æœ¬
    # chunk_overlapä½¿å¾—åˆ†å‰²åçš„æ¯ä¸€ä¸ªchunkéƒ½æœ‰é‡å çš„éƒ¨åˆ†ï¼Œè¿™æ ·å¯ä»¥å‡å°‘é‡è¦ä¸Šä¸‹æ–‡çš„åˆ†å‰²ï¼› add_start_indexä¼šä¸ºæ¯ä¸€ä¸ªchunkåˆ†é…ä¸€ä¸ªç¼–å·
    text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0, add_start_index=True)
    split_docs = text_spliter.split_documents(documents)
    # print(split_docs[0])å¯ä»¥çœ‹åˆ°è¢«åˆ‡å‰²åçš„æ–‡æœ¬
    return split_docs


# ä½¿ç”¨OpenAIçš„embeddingæ¨¡å‹è¦é’±ï¼Œä¹‹åå¯ä»¥æ¢ï¼Œç°åœ¨è¿˜æ˜¯ç”¨æœ¬åœ°çš„å§
# embeddingçš„ä½œç”¨æ˜¯æŠŠæ–‡æœ¬è½¬æ¢åˆ°å‘é‡ç©ºé—´ï¼Œè¿™æ ·å°±å¯ä»¥è¿›è¡Œç›¸ä¼¼åŒ–æ£€ç´¢ç­‰å†…å®¹

"""
    åŠ è½½embedding (ä»huggingfaceä¸Šä¸‹è½½ï¼Œæˆ‘é‡‡ç”¨çš„æ˜¯æœ¬åœ°ä¸‹è½½)
    embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
    "text2vec2": "uer/sbert-base-chinese-nli",
    "text2vec3": "shibing624/text2vec-base-chinese",}
"""

def load_embedding_model(model_name="ernie-tiny"):
    """
    åŠ è½½embeddingæ¨¡å‹
    :param model_name:
    :return:
    """
    encode_kwargs = {"normalize_embeddings": False}
    model_kwargs = {"device": "cuda:0"}
    return HuggingFaceEmbeddings(
        model_name=r"C:\Users\BlackSiao\Desktop\æˆ‘çš„2024\äº†è§£æœ€æ–°çš„AIç¤¾åŒº\OpenAI_test\text2vec",  #æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°'text2vecæ–‡ä»¶å¤¹'
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


# æŠŠå‘é‡å­˜å‚¨åˆ°å‘é‡åº“é‡Œé¢, docsæ˜¯è¢«spliterä¹‹åçš„åˆ—è¡¨ï¼Œembeddingæ˜¯é€‰æ‹©æ¨¡å‹ï¼Œpersist_directoryåœ¨æœ¬åœ°äº§ç”Ÿå‘é‡æ•°æ®åº“
def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    """
    å°†æ–‡æ¡£å‘é‡åŒ–ï¼Œå­˜å…¥å‘é‡æ•°æ®åº“
    :param docs:
    :param embeddings:
    :param persist_directory:
    :return:
    """
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()
    return db


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨æ¥ä½œä¸ºgr.ChatInterface()çš„fnï¼Œhistory[
def predict(message, history):
    # è®¾ç½®æç¤ºè¯ï¼Œå…¶ä½œç”¨æ˜¯è¾“å…¥é—®é¢˜ç»™llmå¤„ç†
    template = """å›ç­”æ¥ä¸‹æ¥çš„é—®é¢˜ï¼Œå¿…é¡»ä»¥ä¸­æ–‡ç»™å‡ºå›ç­”,åŸºäºç»™å‡ºçš„context:
    {context}

    Question: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    # åŠ è½½å¹¶åˆå§‹åŒ–æ¨¡å‹
    model = ChatOpenAI(temperature=1.0, model="gpt-3.5-turbo")
    # å°†llmçš„è¾“å‡ºè½¬æ¢ä¸ºstr
    output_parser = StrOutputParser()
    # Retrieveræ£€ç´¢å‡½æ•° Vector store-backed retriever
    # docs = load_documents()
    # åŠ è½½embeddingæ¨¡å‹
    embedding = load_embedding_model('text2vec3')
    # åŠ è½½æ•°æ®åº“ï¼Œä¸å­˜åœ¨å‘é‡åº“å°±ç”Ÿæˆï¼Œå¦åˆ™ç›´æ¥åŠ è½½
    if not os.path.exists('../VectorStore'):
        documents = load_documents(directory='book')
        db = store_chroma(documents, embedding)
    else:
        db = Chroma(persist_directory='VectorStore', embedding_function=embedding)
    # ä»æ•°æ®åº“ä¸­è·å–ä¸€ä¸ªæ£€ç´¢å™¨ï¼Œç”¨äºä»å‘é‡åº“ä¸­æ£€ç´¢å’Œç»™å®šæ–‡æœ¬ç›¸åŒ¹é…çš„å†…å®¹
    # search_kwargsè®¾ç½®æ£€ç´¢å™¨ä¼šè¿”å›å¤šå°‘ä¸ªåŒ¹é…çš„å‘é‡
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})
    setup_and_retrieval = RunnableParallel(
        {"context": retriever, "question": RunnablePassthrough()}
    )
    # é“¾å¼å†™æ³•
    chain = setup_and_retrieval | prompt | model | output_parser
    response = chain.invoke(message)
    return response


# æœ¬åœ°æ£€ç´¢ï¼ŒåŠ è½½æœ¬åœ°æ–‡ä»¶ä¹Ÿåº”è¯¥å•ç‹¬é¢†å‡ºæ¥å†™ä¸€ä¸ªå‡½æ•°

# UIç•Œé¢
def print_like_dislike(x: gr.LikeData):
    print(x.index, x.value, x.liked)


def add_text(history, text):
    history = history + [(text, None)]
    return history, gr.Textbox(value="", interactive=False)


def add_file(history, file):
    directory = os.path.dirname(file.name)  # æ‹¿åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹
    documents = load_documents(directory)
    embedding = load_embedding_model()
    store_chroma(documents, embedding)   #
    # å°†ä¸´æ—¶ä¸Šä¼ çš„åŠ è½½å¥½ï¼Œå¹¶å­˜åˆ°æ•°æ®åº“é‡Œé¢
    history = history + [((file.name,), None)]
    return history


# å®šä¹‰æ»‘å—ç›‘å¬äº‹ä»¶ï¼Œå½“ç”¨æˆ·æ¾å¼€æ»‘å—æ—¶ï¼Œæ›´æ”¹å¯¹llmæ¨¡å‹çš„è®¾ç½®
def change_temperature(temperature_value):
    # åŠ è½½å¹¶åˆå§‹åŒ–æ¨¡å‹
    model = ChatOpenAI(temperature=temperature_value, model="gpt-3.5-turbo")


def change_maxtoken(token_value):
    # åŠ è½½å¹¶åˆå§‹åŒ–æ¨¡å‹
    model = ChatOpenAI(max_tokens=token_value, model="gpt-3.5-turbo")


def bot(history):
    '''
    historyå‚æ•°æ˜¯ä¸€ä¸ªè®°å½•å¯¹è¯å†å²çš„åˆ—è¡¨ã€‚
    æ¯ä¸ªå†å²è®°å½•éƒ½æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œå…¶ä¸­åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’Œå¯¹åº”çš„æœºå™¨äººå›å¤ã€‚
    åœ¨è¿™ä¸ªåˆ—è¡¨ä¸­ï¼Œæœ€æ–°çš„å¯¹è¯è®°å½•æ€»æ˜¯ä½äºåˆ—è¡¨çš„æœ€åä¸€ä¸ªä½ç½®ï¼Œå› æ­¤history[-1]è¡¨ç¤ºæœ€æ–°çš„å¯¹è¯è®°å½•ã€‚
    '''
    message = history[-1][0]
    # ç”¨æ¥åœ¨ç»ˆç«¯ç›‘è§†ç¨‹åºæœ‰æ— æ­£ç¡®æå–åˆ°ç”¨æˆ·æå‡ºçš„é—®é¢˜
    print(message)
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æˆåŠŸï¼Œå¦‚æœä¸Šä¼ çš„æ˜¯æ–‡ä»¶ï¼Œåˆ™ç”¨æˆ·ä¿¡æ¯å°±æ˜¯å…ƒç»„
    if isinstance(message, tuple):
        response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ï¼"
    else:
        response = predict(message, history)
    # å°†æœ€æ–°å¯¹è¯è®°å½•ä¸­çš„æœºå™¨äººå›å¤éƒ¨åˆ†ç½®ç©º
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.05)
        yield history


with gr.Blocks() as demo:
    # å®šä¹‰èŠå¤©æ¡†
    chatbot = gr.Chatbot(
        [],
        elem_id="chatbot",
        bubble_full_width=False,
        avatar_images=(None, (os.path.join(os.path.dirname(__file__), "avatar.png"))),
        # layoutå¦‚æœä¸º"panel"æ˜¾ç¤ºèŠå¤©æ¡†ä¸ºllmé£æ ¼ï¼Œ"bubbles"æ˜¾ç¤ºä¸ºèŠå¤©æ°”æ³¡
        layout="bubble"
    )
    # å®šä¹‰è¡Œçš„å¸ƒå±€g
    with gr.Row():
        txt = gr.Textbox(
            scale=4,  # è®¾ç½®ä¸ç›¸é‚»å…ƒä»¶å¤§å°çš„æ¯”åˆ—
            show_label=False,
            placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ–è€…ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶",
            container=False,
        )
        # é™å®šä¸Šä¼ æ–‡ä»¶çš„ç±»å‹åªä¸ºtextæ–‡ä»¶
        btn = gr.UploadButton("ğŸ“", file_types=["text"])
        # è®¾ç½®ä¸‰ä¸ªæ»‘å—
    with gr.Column(scale=1):
        temperature_slider = gr.Slider(minimum=0, maximum=2, value=0.7, step=0.1, interactive=True, info="æ¸©åº¦è°ƒèŠ‚æ»‘å—ï¼Œç”¨æ¥æ§åˆ¶llmå›ç­”çš„éšæœºæ€§")
        maxtoken_slider = gr.Slider(minimum=100, maximum=1000, value=500, step=50, interactive=True, info="æ§åˆ¶llmå›ç­”çš„å­—æ•°")

    # è®¾ç½®æäº¤ç”¨æˆ·é—®é¢˜æŒ‰é’®çš„ç›‘å¬äº‹ä»¶
    # é¦–å…ˆè°ƒç”¨add_text()å‡½æ•°å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œéšåä¼ å…¥llmæ¨¡å‹è¿”å›å›ç­”
    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(
        bot, chatbot, chatbot, api_name="bot_response"
    )
    txt_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)
    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
        bot, chatbot, chatbot
    )
    # è®¾ç½®æ»‘å—çš„ç›‘å¬äº‹ä»¶
    temperature_slider.release(change_temperature(temperature_slider.value))
    maxtoken_slider.release(change_maxtoken(maxtoken_slider.value))

    chatbot.like(print_like_dislike, None, None)


demo.queue()
if __name__ == "__main__":
    demo.launch()
