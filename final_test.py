# å®ŒæˆåŸºæœ¬çš„UIç•Œé¢ï¼Œè°ƒè¯•ä½¿å¾—LLMå›ç­”çš„å‡†ç¡®åº¦æé«˜
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_openai import ChatOpenAI
import gradio as gr
import os
import time
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.messages import AIMessage, HumanMessage

# è°ƒç”¨LangChain-smith
os.environ['LANGCHAIN_API_KEY'] = 'ls__57a5dab74800476883643b1813746542'
os.environ['LANGCHAIN_PROJECT'] = 'RAG-Application'
os.environ['LANGCHAIN_TRACING_V2'] = 'true'
os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'
chat_history = []

# æ–‡ä»¶åŠ è½½ï¼Œç›´æ¥åŠ è½½æœ¬åœ°bookæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨æ‹†åˆ†å™¨å°†å…¶æ‹†åˆ†
def load_documents(directory):
    # silent_errorså¯ä»¥è·³è¿‡ä¸èƒ½è§£ç çš„å†…å®¹
    text_loader_kwargs = {'autodetect_encoding': True}
    loader = DirectoryLoader(directory, show_progress=True, silent_errors=True, loader_kwargs=text_loader_kwargs)
    documents = loader.load()

    # åŠ è½½æ–‡æ¡£åï¼Œè¦ä½¿å¾—å†…å®¹å˜å¾—æ˜“äºllmåŠ è½½ï¼Œå°±å¿…é¡»æŠŠé•¿æ–‡æœ¬åˆ‡å‰²æˆä¸€ä¸ªä¸ªçš„å°æ–‡æœ¬
    # chunk_overlapä½¿å¾—åˆ†å‰²åçš„æ¯ä¸€ä¸ªchunkéƒ½æœ‰é‡å çš„éƒ¨åˆ†ï¼Œè¿™æ ·å¯ä»¥å‡å°‘é‡è¦ä¸Šä¸‹æ–‡çš„åˆ†å‰²ï¼› add_start_indexä¼šä¸ºæ¯ä¸€ä¸ªchunkåˆ†é…ä¸€ä¸ªç¼–å·
    text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0, add_start_index=True)
    split_docs = text_spliter.split_documents(documents)
    # print(split_docs[0])å¯ä»¥çœ‹åˆ°è¢«åˆ‡å‰²åçš„æ–‡æœ¬
    return split_docs


# ä½¿ç”¨OpenAIçš„embeddingæ¨¡å‹è¦é’±ï¼Œä¹‹åå¯ä»¥æ¢ï¼Œç°åœ¨è¿˜æ˜¯ç”¨æœ¬åœ°çš„å§
# embeddingçš„ä½œç”¨æ˜¯æŠŠæ–‡æœ¬è½¬æ¢åˆ°å‘é‡ç©ºé—´ï¼Œè¿™æ ·å°±å¯ä»¥è¿›è¡Œç›¸ä¼¼åŒ–æ£€ç´¢ç­‰å†…å®¹

"""
    åŠ è½½embedding (ä»huggingfaceä¸Šä¸‹è½½ï¼Œæˆ‘é‡‡ç”¨çš„æ˜¯æœ¬åœ°ä¸‹è½½)
    embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
    "text2vec2": "uer/sbert-base-chinese-nli",
    "text2vec3": "shibing624/text2vec-base-chinese",}
"""

def load_embedding_model(model_name="ernie-tiny"):
    """
    åŠ è½½embeddingæ¨¡å‹
    :param model_name:
    :return:
    """
    encode_kwargs = {"normalize_embeddings": False}
    model_kwargs = {"device": "cuda:0"}
    return HuggingFaceEmbeddings(
        model_name=r"C:\Users\BlackSiao\Desktop\æˆ‘çš„2024\äº†è§£æœ€æ–°çš„AIç¤¾åŒº\OpenAI_test\text2vec",  #æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°'text2vecæ–‡ä»¶å¤¹'
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


# æŠŠå‘é‡å­˜å‚¨åˆ°å‘é‡åº“é‡Œé¢, docsæ˜¯è¢«spliterä¹‹åçš„åˆ—è¡¨ï¼Œembeddingæ˜¯é€‰æ‹©æ¨¡å‹ï¼Œpersist_directoryåœ¨æœ¬åœ°äº§ç”Ÿå‘é‡æ•°æ®åº“
def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    """
    å°†æ–‡æ¡£å‘é‡åŒ–ï¼Œå­˜å…¥å‘é‡æ•°æ®åº“
    :param docs:
    :param embeddings:
    :param persist_directory:
    :return:
    """
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()
    return db


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨æ¥ä½œä¸ºgr.ChatInterface()çš„fnï¼Œhistory[
def predict(message, history):
    # åŠ è½½å¹¶åˆå§‹åŒ–æ¨¡å‹
    model = ChatOpenAI(temperature=1.0, model="gpt-3.5-turbo")
    # å°†llmçš„è¾“å‡ºè½¬æ¢ä¸ºstr
    output_parser = StrOutputParser()
    # åŠ è½½embeddingæ¨¡å‹
    embedding = load_embedding_model('text2vec3')
    # åŠ è½½æ•°æ®åº“ï¼Œä¸å­˜åœ¨å‘é‡åº“å°±ç”Ÿæˆï¼Œå¦åˆ™ç›´æ¥åŠ è½½
    if not os.path.exists('VectorStore'):
        documents = load_documents(directory='book')
        db = store_chroma(documents, embedding)
    else:
        db = Chroma(persist_directory='VectorStore', embedding_function=embedding)
    # ä»æ•°æ®åº“ä¸­è·å–ä¸€ä¸ªæ£€ç´¢å™¨ï¼Œç”¨äºä»å‘é‡åº“ä¸­æ£€ç´¢å’Œç»™å®šæ–‡æœ¬ç›¸åŒ¹é…çš„å†…å®¹
    # search_kwargsè®¾ç½®æ£€ç´¢å™¨ä¼šè¿”å›å¤šå°‘ä¸ªåŒ¹é…çš„å‘é‡
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})
    setup_and_retrieval = RunnableParallel(
        {"context": retriever, "question": RunnablePassthrough()}
    )

    # è®¾ç½®å­é“¾ï¼Œå¦‚æœç”¨æˆ·çš„æœ€æ–°é—®é¢˜å’Œå‰æ–‡ç›¸å…³ï¼Œäº¤ç”±å­é“¾å¤„ç†ï¼Œåœ¨æç¤ºè¯å†…æ·»åŠ ä¸Šchat_histor
    contextualize_q_system_prompt = """å¦‚æœç”¨æˆ·æœ€æ–°çš„æé—®å†…å®¹å’Œä¹‹å‰çš„å¯¹è¯å†…å®¹
        ç›¸å…³ï¼Œåˆ™é‡æ–°ç¼–æ’ç”¨æˆ·çš„æœ€æ–°æé—®ï¼Œæ·»åŠ ä¸Šä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼›
        å¦‚æœç”¨æˆ·æœ€æ–°çš„æé—®å†…å®¹ä¸æ¶‰åŠåˆ°ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œåˆ™ç›´æ¥è¿”å›è¯¥æé—®å†…å®¹
        ä¸ç”¨å›ç­”æ­¤é—®é¢˜ï¼Œä½ çš„ä»»åŠ¡è¦ä¹ˆé‡æ–°ç¼–æ’è¯¥æé—®ï¼Œè¦ä¹ˆåŸæ ·è¿”å›"""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ]
    )

    # è®¾ç½®ä¸»é“¾çš„å†…å®¹
    qa_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“é—¨å›ç­”é—®é¢˜çš„åŠ©æ‰‹ã€‚ \
        æ“…é•¿ä½¿ç”¨ä»¥ä¸‹çš„Contextä½œä¸ºä¾æ®å›ç­”é—®é¢˜ã€‚ \
        å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå›ç­”ä¸çŸ¥é“å³å¯ï¼Œä¸è¦å°è¯•æé€ ç­”æ¡ˆã€‚ \
        å°½å¯èƒ½è¯¦ç»†ï¼Œå…¨é¢çš„å›ç­”é—®é¢˜ã€‚\
        {context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ]
    )
    # LCELå†™æ³•ï¼Œæ•´åˆå­é“¾å’Œä¸»é“¾
    contextualize_q_chain = contextualize_q_prompt | model | StrOutputParser()

    # åˆ¤æ–­ç”¨æˆ·è¾“å…¥çš„é—®é¢˜æœ‰æ²¡æœ‰æ¶‰åŠä¸Šä¸‹æ–‡
    def contextualized_question(input: dict):
        if input.get("chat_history"):
            return contextualize_q_chain
        else:
            return input["question"]

    # å¯¹è¾“å‡ºçš„contextè¿›è¡Œæ•´ç†
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
            RunnablePassthrough.assign(
                context=contextualized_question | retriever | format_docs
            )
            | qa_prompt
            | model
            | output_parser
    )
    question = message
    ai_msg = rag_chain.invoke({"question": question, "chat_history": chat_history})
    chat_history.extend([HumanMessage(content=question), ai_msg])
    return ai_msg

# UIç•Œé¢
def print_like_dislike(x: gr.LikeData):
    print(x.index, x.value, x.liked)


def add_text(history, text):
    history = history + [(text, None)]

    return history, gr.Textbox(value="", interactive=False)


def add_file(history, file):
    directory = os.path.dirname(file.name)  # æ‹¿åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹
    documents = load_documents(directory)
    embedding = load_embedding_model()
    store_chroma(documents, embedding)   #
    # å°†ä¸´æ—¶ä¸Šä¼ çš„åŠ è½½å¥½ï¼Œå¹¶å­˜åˆ°æ•°æ®åº“é‡Œé¢
    history = history + [((file.name,), None)]
    return history


# å®šä¹‰æ»‘å—ç›‘å¬äº‹ä»¶ï¼Œå½“ç”¨æˆ·æ¾å¼€æ»‘å—æ—¶ï¼Œæ›´æ”¹å¯¹llmæ¨¡å‹çš„è®¾ç½®
def change_temperature(temperature_value):
    # åŠ è½½å¹¶åˆå§‹åŒ–æ¨¡å‹
    model = ChatOpenAI(temperature=temperature_value, model="gpt-3.5-turbo")


def change_maxtoken(token_value):
    # åŠ è½½å¹¶åˆå§‹åŒ–æ¨¡å‹
    model = ChatOpenAI(max_tokens=token_value, model="gpt-3.5-turbo")


def bot(history):
    '''
    historyå‚æ•°æ˜¯ä¸€ä¸ªè®°å½•å¯¹è¯å†å²çš„åˆ—è¡¨ã€‚
    æ¯ä¸ªå†å²è®°å½•éƒ½æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œå…¶ä¸­åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’Œå¯¹åº”çš„æœºå™¨äººå›å¤ã€‚
    åœ¨è¿™ä¸ªåˆ—è¡¨ä¸­ï¼Œæœ€æ–°çš„å¯¹è¯è®°å½•æ€»æ˜¯ä½äºåˆ—è¡¨çš„æœ€åä¸€ä¸ªä½ç½®ï¼Œå› æ­¤history[-1]è¡¨ç¤ºæœ€æ–°çš„å¯¹è¯è®°å½•ã€‚
    '''
    message = history[-1][0]
    # ç”¨æ¥åœ¨ç»ˆç«¯ç›‘è§†ç¨‹åºæœ‰æ— æ­£ç¡®æå–åˆ°ç”¨æˆ·æå‡ºçš„é—®é¢˜
    print(message)
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æˆåŠŸï¼Œå¦‚æœä¸Šä¼ çš„æ˜¯æ–‡ä»¶ï¼Œåˆ™ç”¨æˆ·ä¿¡æ¯å°±æ˜¯å…ƒç»„
    if isinstance(message, tuple):
        response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ï¼"
    else:
        response = predict(message, history)
    # å°†æœ€æ–°å¯¹è¯è®°å½•ä¸­çš„æœºå™¨äººå›å¤éƒ¨åˆ†ç½®ç©º
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.05)
        yield history


with gr.Blocks() as demo:
    # å®šä¹‰èŠå¤©æ¡†
    chatbot = gr.Chatbot(
        [],
        elem_id="chatbot",
        bubble_full_width=False,
        avatar_images=(None, (os.path.join(os.path.dirname(__file__), "avatar.png"))),
        # layoutå¦‚æœä¸º"panel"æ˜¾ç¤ºèŠå¤©æ¡†ä¸ºllmé£æ ¼ï¼Œ"bubbles"æ˜¾ç¤ºä¸ºèŠå¤©æ°”æ³¡
        layout="bubble"
    )
    # å®šä¹‰è¡Œçš„å¸ƒå±€g
    with gr.Row():
        txt = gr.Textbox(
            scale=4,  # è®¾ç½®ä¸ç›¸é‚»å…ƒä»¶å¤§å°çš„æ¯”åˆ—
            show_label=False,
            placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ–è€…ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶",
            container=False,
        )
        # é™å®šä¸Šä¼ æ–‡ä»¶çš„ç±»å‹åªä¸ºtextæ–‡ä»¶
        btn = gr.UploadButton("ğŸ“", file_types=["text"])
        # è®¾ç½®ä¸‰ä¸ªæ»‘å—
    with gr.Column(scale=1):
        temperature_slider = gr.Slider(minimum=0, maximum=2, value=0.7, step=0.1, interactive=True, info="æ¸©åº¦è°ƒèŠ‚æ»‘å—ï¼Œç”¨æ¥æ§åˆ¶llmå›ç­”çš„éšæœºæ€§")
        maxtoken_slider = gr.Slider(minimum=100, maximum=1000, value=500, step=50, interactive=True, info="æ§åˆ¶llmå›ç­”çš„å­—æ•°")

    # è®¾ç½®æäº¤ç”¨æˆ·é—®é¢˜æŒ‰é’®çš„ç›‘å¬äº‹ä»¶
    # é¦–å…ˆè°ƒç”¨add_text()å‡½æ•°å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œéšåä¼ å…¥llmæ¨¡å‹è¿”å›å›ç­”
    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(
        bot, chatbot, chatbot, api_name="bot_response"
    )
    txt_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)
    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
        bot, chatbot, chatbot
    )
    # è®¾ç½®æ»‘å—çš„ç›‘å¬äº‹ä»¶
    temperature_slider.release(change_temperature(temperature_slider.value))
    maxtoken_slider.release(change_maxtoken(maxtoken_slider.value))

    chatbot.like(print_like_dislike, None, None)


demo.queue()
if __name__ == "__main__":
    demo.launch()