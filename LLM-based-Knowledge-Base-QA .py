from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_openai import ChatOpenAI
import gradio as gr
import os
import time
from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_core.messages import AIMessage, HumanMessage
from langchain_community.llms import Tongyi

# æ–‡å¿ƒä¸€è¨€é…ç½®
WENXIN_APP_Key = "sKzLrpmNHh4iHVGqwmntUurg"
WENXIN_APP_SECRET = "DtHAE7441OlC1g0MsWoC3eMt6UVSr1zf"

# é€šè¯‘åƒé—®çš„é…ç½®
DASHSCOPE_API_KEY= "sk-8b5a0d0d6c8a41b6ab41a5553808be85"

# æ–‡ä»¶åŠ è½½ï¼Œç›´æ¥åŠ è½½æœ¬åœ°bookæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨æ‹†åˆ†å™¨å°†å…¶æ‹†åˆ†
def load_documents(directory):
    # silent_errorså¯ä»¥è·³è¿‡ä¸èƒ½è§£ç çš„å†…å®¹
    text_loader_kwargs = {'autodetect_encoding': True}
    loader = DirectoryLoader(directory, show_progress=True,silent_errors=True, loader_kwargs=text_loader_kwargs, )
    documents = loader.load()
    # åŠ è½½æ–‡æ¡£åï¼Œè¦ä½¿å¾—å†…å®¹å˜å¾—æ˜“äºllmåŠ è½½ï¼Œå°±å¿…é¡»æŠŠé•¿æ–‡æœ¬åˆ‡å‰²æˆä¸€ä¸ªä¸ªçš„å°æ–‡æœ¬
    # chunk_overlapä½¿å¾—åˆ†å‰²åçš„æ¯ä¸€ä¸ªchunkéƒ½æœ‰é‡å çš„éƒ¨åˆ†ï¼Œè¿™æ ·å¯ä»¥å‡å°‘é‡è¦ä¸Šä¸‹æ–‡çš„åˆ†å‰²ï¼› add_start_indexä¼šä¸ºæ¯ä¸€ä¸ªchunkåˆ†é…ä¸€ä¸ªç¼–å·
    text_spliter = CharacterTextSplitter(chunk_size=50, chunk_overlap=5, add_start_index=True)
    split_docs = text_spliter.split_documents(documents)
    # split_docsæ˜¯ä¸€ä¸ªåŒ…å«äº†å¤šä¸ªchunkçš„æ•°ç»„ï¼Œprint(split_docs[0])å¯ä»¥çœ‹åˆ°è¢«åˆ‡å‰²åçš„æ–‡æœ¬
    return split_docs

"""
    åŠ è½½embedding (ä»huggingfaceä¸Šä¸‹è½½ï¼Œæˆ‘é‡‡ç”¨çš„æ˜¯æœ¬åœ°ä¸‹è½½)
    embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
    "text2vec2": "uer/sbert-base-chinese-nli",
    "text2vec3": "shibing624/text2vec-base-chinese",}
"""
# @software{text2vec,
#   author = {Xu Ming},
#   title = {text2vec: A Tool for Text to Vector},
#   year = {2022},
#   url = {https://github.com/shibing624/text2vec},
# }


def load_embedding_model(model_name="ernie-tiny"):
    """
    åŠ è½½embeddingæ¨¡å‹
    :param model_name:
    :return:
    """
    encode_kwargs = {"normalize_embeddings": False}
    model_kwargs = {"device": "cuda:0"}
    return HuggingFaceEmbeddings(
        model_name=r"C:\Users\BlackSiao\Desktop\æˆ‘çš„2024\äº†è§£æœ€æ–°çš„AIç¤¾åŒº\OpenAI_test\text2vec",  #æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°'text2vecæ–‡ä»¶å¤¹'
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


# æŠŠå‘é‡å­˜å‚¨åˆ°å‘é‡åº“é‡Œé¢, åœ¨æœ¬åœ°äº§ç”Ÿå‘é‡æ•°æ®åº“"VectorStore"
def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    """
    å°†æ–‡æ¡£å‘é‡åŒ–ï¼Œå­˜å…¥å‘é‡æ•°æ®åº“
    :param docs:
    :param embeddings:
    :param persist_directory:
    :return:
    """
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()
    return db

# è®¾ç½®ä¸ºå…¨å±€å˜é‡
model_name = 1
temperature_value=0.7
token_value=5000
ret_value=2

# åˆå§‹åŒ–è®¾ç½®ï¼Œä¼˜åŒ–ä»£ç è¿è¡Œé€Ÿåº¦
chat_history = []
# åŠ è½½å¹¶åˆå§‹åŒ–æ¨¡å‹
model = ChatOpenAI(model="gpt-3.5-turbo", temperature=1.0)
# å°†llmçš„è¾“å‡ºè½¬æ¢ä¸ºstr
output_parser = StrOutputParser()
# åŠ è½½embeddingæ¨¡å‹
embedding = load_embedding_model('text2vec3')
# åŠ è½½æ•°æ®åº“ï¼Œä¸å­˜åœ¨å‘é‡åº“å°±ç”Ÿæˆï¼Œå¦åˆ™ç›´æ¥åŠ è½½
if not os.path.exists('VectorStore'):
    documents = load_documents(directory='Answer')
    db = store_chroma(documents, embedding)
else:
    db = Chroma(persist_directory='VectorStore', embedding_function=embedding)
# ä»æ•°æ®åº“ä¸­è·å–ä¸€ä¸ªæ£€ç´¢å™¨ï¼Œç”¨äºä»å‘é‡åº“ä¸­æ£€ç´¢å’Œç»™å®šæ–‡æœ¬ç›¸åŒ¹é…çš„å†…å®¹
# search_kwargsè®¾ç½®æ£€ç´¢å™¨ä¼šè¿”å›å¤šå°‘ä¸ªåŒ¹é…çš„å‘é‡
retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})
setup_and_retrieval = RunnableParallel(
    {"context": retriever, "question": RunnablePassthrough()}
)
# è®¾ç½®å­é“¾ï¼Œå¦‚æœç”¨æˆ·çš„æœ€æ–°é—®é¢˜å’Œå‰æ–‡ç›¸å…³ï¼Œäº¤ç”±å­é“¾å¤„ç†ï¼Œåœ¨æç¤ºè¯å†…æ·»åŠ ä¸Šchat_history
contextualize_q_system_prompt = """å¦‚æœç”¨æˆ·æœ€æ–°çš„æé—®å†…å®¹å’Œä¹‹å‰çš„å¯¹è¯å†…å®¹
    ç›¸å…³ï¼Œåˆ™é‡æ–°ç¼–æ’ç”¨æˆ·çš„æœ€æ–°æé—®ï¼Œæ·»åŠ ä¸Šä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼›
    å¦‚æœç”¨æˆ·æœ€æ–°çš„æé—®å†…å®¹ä¸æ¶‰åŠåˆ°ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œåˆ™ç›´æ¥è¿”å›è¯¥æé—®å†…å®¹
    ä¸ç”¨å›ç­”æ­¤é—®é¢˜ï¼Œä½ çš„ä»»åŠ¡è¦ä¹ˆé‡æ–°ç¼–æ’è¯¥æé—®ï¼Œè¦ä¹ˆåŸæ ·è¿”å›"""
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
)

# è®¾ç½®ä¸»é“¾çš„å†…å®¹
qa_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“é—¨å›ç­”é—®é¢˜çš„åŠ©æ‰‹ã€‚ \
    æ“…é•¿ä½¿ç”¨ä»¥ä¸‹çš„Contextä½œä¸ºä¾æ®å›ç­”é—®é¢˜ã€‚ \
    å¦‚æœç”¨æˆ·çš„é—®é¢˜å’ŒContextæ— å…³ï¼Œåˆ™å¿½ç•¥Contextå¹¶å°è¯•å›ç­”é—®é¢˜ã€‚ \
    å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå›ç­”ä¸çŸ¥é“å³å¯ï¼Œä¸è¦å°è¯•æé€ ç­”æ¡ˆã€‚ \
    å°½å¯èƒ½è¯¦ç»†ï¼Œå…¨é¢çš„å›ç­”é—®é¢˜ã€‚\
    {context}"""
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
)
# æ•´åˆå­é“¾å’Œä¸»é“¾
contextualize_q_chain = contextualize_q_prompt | model | StrOutputParser()

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨æ¥ä½œä¸ºgr.ChatInterface()çš„fnï¼Œhistory[]
def predict(message, history):
    # åˆ¤æ–­ç”¨æˆ·è¾“å…¥çš„é—®é¢˜æœ‰æ²¡æœ‰æ¶‰åŠä¸Šä¸‹æ–‡
    def contextualized_question(input: dict):
        if input.get("chat_history"):
            return contextualize_q_chain
        else:
            return input["question"]

    # å¯¹è¾“å‡ºçš„contextè¿›è¡Œæ•´ç†
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
            RunnablePassthrough.assign(
                context=contextualized_question | retriever | format_docs
            )
            | qa_prompt
            | model
            | output_parser
    )
    question = message
    ai_msg = rag_chain.invoke({"question": question, "chat_history": chat_history})
    chat_history.extend([HumanMessage(content=question), ai_msg])
    return ai_msg


# UIç•Œé¢
def print_like_dislike(x: gr.LikeData):
    print(x.index, x.value, x.liked)


def add_text(history, text):
    history = history + [(text, None)]

    return history, gr.Textbox(value="", interactive=False)

def add_file(history, file):
    directory = os.path.dirname(file.name)  # æ‹¿åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹
    documents = load_documents(directory)
    embedding = load_embedding_model()
    store_chroma(documents, embedding)   #
    # å°†ä¸´æ—¶ä¸Šä¼ çš„åŠ è½½å¥½ï¼Œå¹¶å­˜åˆ°æ•°æ®åº“é‡Œé¢
    history = history + [((file.name,), None)]
    return history


# å®šä¹‰æ¨¡å‹çš„åˆ‡æ¢åŠå…¶å†…éƒ¨å‚æ•°è®¾è®¡
def model_change_handler(model_name, temperature_value, token_value, ret_value):
    global model
    if model_name == 1:
        model = ChatOpenAI(model="gpt-3.5-turbo",
                           temperature=temperature_value,
                           max_tokens=token_value,
                           max_retries=ret_value)
    # elif model_name == 2:
    #     model = Wenxin(
    #         model="ernie-bot-turbo",
    #         baidu_api_key=WENXIN_APP_Key,
    #         baidu_secret_key=WENXIN_APP_SECRET,
    #         temperature=temperature_value,
    #         max_tokens=token_value,
    #         max_retries=ret_value,
    #         verbose=True,
    #     )
    elif model_name == 3:
        model = Tongyi(
            model="qwen-turbo",
            dashscope_api_key=DASHSCOPE_API_KEY,
            temperature=temperature_value,
            max_tokens=token_value,
            max_retries=ret_value,
            verbose=True,
        )
    elif model_name == 4:
        model = Tongyi(
            model="llama3-8b-instruct",
            dashscope_api_key=DASHSCOPE_API_KEY,
            verbose=True,
            temperature=temperature_value,
            max_tokens=token_value,
            max_retries=ret_value
        )


def bot(history):
    '''
    historyå‚æ•°æ˜¯ä¸€ä¸ªè®°å½•å¯¹è¯å†å²çš„åˆ—è¡¨ã€‚
    æ¯ä¸ªå†å²è®°å½•éƒ½æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œå…¶ä¸­åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’Œå¯¹åº”çš„æœºå™¨äººå›å¤ã€‚
    åœ¨è¿™ä¸ªåˆ—è¡¨ä¸­ï¼Œæœ€æ–°çš„å¯¹è¯è®°å½•æ€»æ˜¯ä½äºåˆ—è¡¨çš„æœ€åä¸€ä¸ªä½ç½®ï¼Œå› æ­¤history[-1]è¡¨ç¤ºæœ€æ–°çš„å¯¹è¯è®°å½•ã€‚
    '''
    message = history[-1][0]
    # ç”¨æ¥åœ¨ç»ˆç«¯ç›‘è§†ç¨‹åºæœ‰æ— æ­£ç¡®æå–åˆ°ç”¨æˆ·æå‡ºçš„é—®é¢˜
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æˆåŠŸï¼Œå¦‚æœä¸Šä¼ çš„æ˜¯æ–‡ä»¶ï¼Œåˆ™ç”¨æˆ·ä¿¡æ¯å°±æ˜¯å…ƒç»„
    if isinstance(message, tuple):
        response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼"
    else:
        response = predict(message, history)
    # å°†æœ€æ–°å¯¹è¯è®°å½•ä¸­çš„æœºå™¨äººå›å¤éƒ¨åˆ†ç½®ç©º
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.05)
        yield history


with gr.Blocks(theme='NoCrypt/miku') as demo:
    gr.Markdown("æ¬¢è¿ä½¿ç”¨æœ¬çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ")
    # å®šä¹‰èŠå¤©æ¡†
    chatbot = gr.Chatbot(
        [],
        elem_id="chatbot",
        bubble_full_width=False,
        avatar_images=(None, (os.path.join(os.path.dirname(__file__), "avatar.png"))),
        # layoutå¦‚æœä¸º"panel"æ˜¾ç¤ºèŠå¤©æ¡†ä¸ºllmé£æ ¼ï¼Œ"bubbles"æ˜¾ç¤ºä¸ºèŠå¤©æ°”æ³¡
        layout="bubble",
        show_copy_button=True,
    )
    # å®šä¹‰è¡Œçš„å¸ƒå±€g
    with gr.Row():
        txt = gr.Textbox(
            scale=4,  # è®¾ç½®ä¸ç›¸é‚»å…ƒä»¶å¤§å°çš„æ¯”åˆ—
            show_label=False,
            placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ–è€…ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶",
            container=False,
        )
        # æäº¤æŒ‰é’®
        submit_btn = gr.Button("æäº¤")
        # æ¸…é›¶æŒ‰é’®, å®šä¹‰å¾…ä¼šè¦æ¸…é›¶çš„ç»„ä»¶
        clear_btn = gr.ClearButton([chatbot, txt], value="æ¸…é™¤å†å²å¯¹è¯")
        # é™å®šä¸Šä¼ æ–‡ä»¶çš„ç±»å‹åªä¸ºtextæ–‡ä»¶
        btn = gr.UploadButton("ğŸ“", file_types=["text"])
    with gr.Accordion("ç”¨æˆ·è‡ªå®šä¹‰å‚æ•°è®¾è®¡"):
        # ä¸ºchatbotæ·»åŠ ç¤ºä¾‹
        gr.Examples(
            examples=[
                ["ä½ å¥½,ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±å§"],
                ["ç®€å•ä»‹ç»ä¸€ä¸‹éª†é©¼ç¥¥å­"],
                ["å„’æ—å¤–å²è®²çš„æ˜¯ä»€ä¹ˆï¼Ÿ"],
                ["å¥¶æ²¹è‰è“è›‹ç³•æ€ä¹ˆåˆ¶ä½œï¼Ÿ"]
            ],
            inputs=txt)
        # è®¾ç½®ä¸‹æ‹‰èœå•ï¼Œæ›´æ”¹ä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹åŸºåº•
        model_change = gr.Dropdown(
            choices=[("GPT-3.5", 1), ("æ–‡å¿ƒä¸€è¨€", 2), ("é€šè¯‘åƒé—®", 3), ("LLam", 4)],
            value=1,
            label="å¤§è¯­è¨€æ¨¡å‹",
            info="é€‰æ‹©æ‚¨æƒ³ä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹åŸºåº§!"
        )
        temp_slider = gr.Slider(minimum=0, maximum=2, value=0.7, step=0.1, label="æ¸©åº¦è°ƒèŠ‚", interactive=True,
                                info="æ§åˆ¶å¤§è¯­è¨€æ¨¡å‹å›ç­”çš„éšæœºæ€§")
        max_slider = gr.Slider(minimum=100, maximum=1000, value=500, step=50, label="æœ€å¤§å›ç­”å­—æ•°", interactive=True,
                               info="æ§åˆ¶è¿”å›ç­”æ¡ˆçš„æœ€å¤§å­—æ•°")
        ret_slider = gr.Slider(minimum=0, maximum=2, value=1, step=1, label="æœ¬åœ°æ£€ç´¢", interactive=True,
                               info="åœ¨å“åº”å¤±è´¥åï¼Œæœ€å¤§çš„é‡è¯•æ¬¡æ•°")
    # è®¾ç½®æäº¤ç”¨æˆ·é—®é¢˜æŒ‰é’®çš„ç›‘å¬äº‹ä»¶
    # é¦–å…ˆè°ƒç”¨add_text()å‡½æ•°å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œéšåä¼ å…¥llmæ¨¡å‹è¿”å›å›ç­”
    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(
        bot, chatbot, chatbot, api_name="bot_response"
    )
    txt_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)
    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
        bot, chatbot, chatbot
    )

    # å°†æäº¤æŒ‰é’®çš„ç›‘å¬äº‹ä»¶ä¸æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ç»‘å®š
    submit_msg = submit_btn.click(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(
        bot, chatbot, chatbot, api_name="bot_response")
    submit_msg.then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)

    # release ç›‘å¬å™¨ï¼Œå½“ç”¨æˆ·æ¾å¼€è°ƒèŠ‚æ»‘å—çš„é¼ æ ‡æ—¶è§¦å‘ï¼Œå°†æ­¤æ—¶çš„æ»‘å—å€¼ä¼ é€’ä¼ ç»™å¯¹åº”çš„fn
    tem_release = temp_slider.release(model_change_handler,
                                      inputs=[model_change, temp_slider, max_slider, ret_slider])
    max_release = max_slider.release(model_change_handler,
                                     inputs=[model_change, temp_slider, max_slider, ret_slider])
    ret_release = ret_slider.release(model_change_handler,
                                     inputs=[model_change, temp_slider, max_slider, ret_slider])

    # ä¸‹æ‹‰æ¡çš„ç›‘å¬äº‹ä»¶,åœ¨åˆ‡æ¢å¤§è¯­è¨€æ¨¡å‹ä¹‹åå¯¹è¯æ¡†æœ‰å¯¹åº”æ˜¾ç¤º
    model_change.select(model_change_handler,
                        inputs=[model_change, temp_slider, max_slider, ret_slider])

    chatbot.like(print_like_dislike, None, None)

demo.queue()
if __name__ == "__main__":
    demo.launch(share=True)